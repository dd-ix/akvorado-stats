#!/usr/bin/env python3

import clickhouse_connect
from collections import namedtuple
import math
import numpy as np
import argparse
import os
import datetime
import configparser
import platform
import requests
import yaml
import logging


def setup_logging(args):
    logger = logging.getLogger("akvo-top-asn")

    if args.debug:
        logger.setLevel(logging.DEBUG)
    elif args.quiet:
        logger.setLevel(logging.WARNING)
    else:
        logger.setLevel(logging.INFO)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)

    # create formatter
    formatter = logging.Formatter('%(levelname)s - %(message)s')

    # add formatter to ch
    ch.setFormatter(formatter)

    # add ch to logger
    logger.addHandler(ch)

    return logger


def check_conf_filename(fn):
    """
    Some simple (Race condition!) config filename checks to create
    polite error messages.
    """
    if not os.path.exists(fn):
        raise argparse.ArgumentTypeError(f"{fn} does not exist")
    if not os.path.isfile(fn):
        raise argparse.ArgumentTypeError(f"{fn} is not a file")

    return fn


def check_time_stamp(ts):
    return datetime.datetime.fromisoformat(ts)


# Instantiate the parser
parser = argparse.ArgumentParser(description="Akvorado top ASN stats")
parser.add_argument(
    "-c", "--config", type=check_conf_filename, default="akvo-top-asn.conf", help="configuration filename"
)
parser.add_argument("--starttime", type=check_time_stamp)
parser.add_argument("--endtime", type=check_time_stamp)

parser.add_argument("--days", type=int, default=0)
parser.add_argument("--weeks", type=int, default=1)
parser.add_argument("--months", type=int, default=0)

parser.add_argument("--filename", default=argparse.SUPPRESS, help="filename strftime pattern for saving and uploading results" )
group = parser.add_argument_group("export actions")
group.add_argument("--save", action="store_true", help="save result to local file")
group.add_argument("--upload", action="store_true", help="upload result to remote http url")
group.add_argument("--print", action="store_true", help="print result to stdout")


# logging settings
logging_group = parser.add_mutually_exclusive_group()
logging_group.add_argument("--debug", action="store_true", help="enable debug logging")
logging_group.add_argument("--quiet", action="store_true", help="supress informational logging")

args = parser.parse_args()
logger = setup_logging(args)

if not (args.print or args.upload or args.save):
  parser.error("You need to specify at least one action!")

# prepare defaults & read configuration
config = configparser.ConfigParser(interpolation=None)
config.add_section('meta')
config['meta']['source'] = platform.node()
config.add_section('asn')
config['asn']['local'] = ''
config['asn']['filter'] = ''
config.add_section('clickhouse')
config["clickhouse"]["host"] = "localhost"
config.add_section('upload')
config.read(args.config)

if not 'asn' in config['meta']:
    logger.error("config option meta.asn is not set")
    exit(1)

starttime = None
endtime = None


# if no starttime is given we will take a timestamp thats in the past by some default offset
if args.starttime is None:
    delta = datetime.timedelta(
        days=args.days + args.weeks * 7 + args.months * 30,
    )

    starttime = datetime.datetime.utcnow() - delta
else:
    starttime = args.starttime

# if no endtime is given we take the current time as endtime
if args.endtime is None:
    endtime = datetime.datetime.utcnow()
else:
    endtime = args.endtime

time_range = [starttime.strftime("%Y-%m-%d %H:%M:%S"), endtime.strftime("%Y-%m-%d %H:%M:%S")]
si_prefixes = ["", "K", "M", "G"]
directions = ["Out", "In"]


def format_bps(n):
    """
    Format value with bps unit and SI prefix.
    """
    idx = max(0, min(len(si_prefixes) - 1, int(math.floor(0 if n == 0 else math.log10(abs(n)) / 3))))

    return "{:.0f}{}bps".format(n / 10 ** (3 * idx), si_prefixes[idx])


def get_connection(config):
    """
    Create a clickhouse client instance based on the config
    parameters of the [clickhouse] section.
    """
    params = {"host": config["clickhouse"]["host"]}

    # add optional parameters
    for param_name, param_type in [
        ("secure", bool),
        ("port", int),
        ("username", str),
        ("password", str),
    ]:
        if param_name in config["clickhouse"]:
            try:
                params[param_name] = param_type(config["clickhouse"][param_name])
            except ValueError as ex:
                print(f"Bad config value for clickhouse.{param_name}: {ex}")
                exit(1)

    logger.info("connecting to clickhouse at %s", params["host"])
    return clickhouse_connect.get_client(**params)


def query_clickhouse(time_range, direction):
    query = f"""
        WITH
        source AS (SELECT * FROM flows_5m0s SETTINGS asterisk_include_alias_columns = 1),
        rows AS (SELECT SrcAS FROM source WHERE TimeReceived BETWEEN toDateTime('{time_range[0]}', 'UTC') AND toDateTime('{time_range[1]}', 'UTC') AND (InIfBoundary = 'external') GROUP BY SrcAS ORDER BY SUM(Bytes) DESC LIMIT 50)
        SELECT 1 AS axis, * FROM (
        SELECT
        toStartOfInterval(TimeReceived + INTERVAL 900 second, INTERVAL 900 second) - INTERVAL 900 second AS time,
        SUM(Bytes*SamplingRate*8)/900 AS xps,
        if((SrcAS) IN rows, [concat(toString(SrcAS), ': ', dictGetOrDefault('asns', 'name', SrcAS, '???'))], ['0: Other']) AS dimensions
        FROM source
        WHERE TimeReceived BETWEEN toDateTime('{time_range[0]}', 'UTC') AND toDateTime('{time_range[1]}', 'UTC') AND ({direction}IfBoundary = 'external')
        GROUP BY time, dimensions
        ORDER BY time WITH FILL
        FROM toDateTime('{time_range[0]}', 'UTC')
        TO toDateTime('{time_range[1]}', 'UTC') + INTERVAL 1 second
        STEP 900
        INTERPOLATE (dimensions AS ['0: Other']))
        """

    logger.debug("clickhouse query: %s", query)
    return client.query(query).result_rows


def query_org(asn: int):
    query = f"SELECT name FROM asns WHERE asn = '{asn}'"

    logger.debug("clickhouse query: %s", query)
    rows = client.query(query).result_rows

    if not rows:
        return None
    else:
        return rows[0][0]


# connect to clickhouse
try:
    client = get_connection(config)
except clickhouse_connect.driver.exceptions.DatabaseError as ex:
    logger.error("Failed to connect to clickhouse: %s", ex)
    exit(2)


# get source ASN
source_asn = int(config['meta']['asn'])
source_org = query_org(source_asn)

# parse local_asn
local_asn = {}
asns = config['asn']['local'].split(',')
logger.info("lookup local asn: %s", ", ".join(asns))
for asn in asns:
    if asn:
        asn = int(asn)
        local_asn[asn] = query_org(asn)

# parse filter_asn
filter_asn = {}
asns = config['asn']['filter'].split(',')
logger.info("lookup filter asn: %s", ", ".join(asns))
for asn in asns:
    if asn:
        asn = int(asn)
        filter_asn[asn] = query_org(asn)


logger.info("query time range: %s - %s", *time_range)

# build list of bandwidths per ASN
asn_xps = {}
for direction in directions:
    for axis, ts, xps, _asn in query_clickhouse(time_range, direction):
        asn = _asn[0]
        if asn in asn_xps:
            if direction in asn_xps[asn]:
                asn_xps[asn][direction].append(xps)
            else:
                asn_xps[asn][direction] = [xps]
        else:
            asn_xps[asn] = {direction: [xps]}

logger.info("building statistics...")


# build stats for each ASN
asn_stats = {}
for _asn, xps in asn_xps.items():
    asn, org = map(str.strip, _asn.split(':', 1))
    asn = int(asn)

    # skip 'Other', local or filtered asns
    if asn == 0:
        continue
    if asn in local_asn:
        continue
    if asn in filter_asn:
        continue

    stats = {
        'org': org,
    }
    for direction in directions:
        d = direction.lower()

        if direction in xps:
            arr = np.array(xps[direction])

            stats[f"{d}_avg"] = np.mean(arr)
            stats[f"{d}_p95"] = np.percentile(arr, 95)
            stats[f"{d}_max"] = np.max(arr)
        else:
            stats[f"{d}_avg"] = 0
            stats[f"{d}_p95"] = 0
            stats[f"{d}_max"] = 0

    asn_stats[asn] = stats

if args.save or args.upload:
    fn = starttime.strftime(getattr(args, 'filename', config['upload'].get('filename', 'export-%Y-W%W.yml')))

    # dump result to string
    yaml.add_representer(np.float64, lambda dumper, data: dumper.represent_scalar('tag:yaml.org,2002:float', str(data)))
    result = yaml.dump({
        'meta': {
            'from': time_range[0],
            'to': time_range[1],
            'asn': source_asn,
            'org': source_org,
        },
        'local_asn': local_asn,
        'filter_asn': filter_asn,
        'top_peers': asn_stats,
    })

    # save result to local file
    if args.save:
        logger.info("saving to file: %s", fn)
        with open(fn, 'w') as fh:
            fh.write(result)

    # upload result to http remote location
    if args.upload:
        params = {
            "headers": {
                "X-Requested-With": "XMLHttpRequest",
            },
            "data": result,
        }

        if "username" in config["upload"] or "password" in config["upload"]:
            params["auth"] = (config["upload"].get("username"), config["upload"].get("password"))

        url = config["upload"]["url"] + "/" + fn
        logger.info("uploading to: %s", url)
        response = requests.put(url, **params)
        if response.status_code in range(200, 299):
            logger.info("upload succeeded")
        else:
            logging.error("upload failed: %d - %s", response.status_code, response.text)


# print to stdout
if args.print:
    # poor-mans table headers
    print("{asn: <7} {org: <36}".format(asn="ASN", org="ORG"), end="")
    columns = []
    for direction in directions:
        for metric in ["avg", "p95", "max"]:
            columns.append(f"{{{direction.lower()}_{metric}: >9}}")
            print(" {metric: <9}".format(metric=f"{direction.lower()}_{metric}"), end="")
    print()

    # sort & print ASN stats
    for asn, stats in sorted(asn_stats.items(), key=lambda i: i[1]["out_p95"] + i[1]["in_p95"], reverse=True):
        print(f'{{asn: <7}} {{org: <36}} {" ".join(columns)}'.format(asn=asn, org=stats['org'], **{k: format_bps(v) for k, v in stats.items() if k != "org"}))
