#!/usr/bin/env python3

import argparse
import datetime
from pdf_reports import pug_to_html, write_report
import fsspec
import math
import os
from pathlib import Path
import re
import yaml

source_dir = Path(__file__).resolve().parent

P95_CUT_OFF = 10_000_000
SI_PREFIXES = ["", "K", "M", "G"]
SIBLING_ASN_SOURCES = {
  "Akamai": "https://raw.githubusercontent.com/SecOps-Institute/Akamai-ASN-and-IPs-List/master/akamai_asn_list.lst",
  "Amazon": f"{source_dir}/siblings/amazon-asn.lst",
  "Apple": f"{source_dir}/siblings/apple-asn.lst",
  "DigitalOcean": "https://raw.githubusercontent.com/SecOps-Institute/Digitalocean-ASN-and-IPs-List/master/digitalocean_asn_list.lst",
  "Facebook": "https://raw.githubusercontent.com/SecOps-Institute/FacebookIPLists/master/facebook_asn_list.lst",
  "Google": f"{source_dir}/siblings/google-asn.lst",
  "LinkedIn": "https://raw.githubusercontent.com/SecOps-Institute/LinkedInIPLists/master/linkedin_asn_list.lst",
  "Microsoft": f"{source_dir}/siblings/microsoft-asn.lst",
}

def format_bps(n):
  """
  Format value with bps unit and SI prefix.
  """
  idx = max(0, min(len(SI_PREFIXES) - 1, int(math.floor(0 if n == 0 else math.log10(abs(n)) / 3))))

  return "{:.1f}&thinsp;{}bps".format(n / 10 ** (3 * idx), SI_PREFIXES[idx])

def slugify(s):
  s = re.sub(r'[^\w\s-]', '', s)
  return re.sub(r'[-\s]+', '-', s).strip('-_')

parser = argparse.ArgumentParser(
    description="Akvorado Traffic Analysis Report Builder")
parser.add_argument("-i","--individual", action="store_true", help="generate individual reports")
parser.add_argument("-w","--watermark", type=str, default="example", help="watermark text (default: example)")
parser.add_argument("--cache", type=str, default=".cache", help="caching directory (default: .cache)")
parser.add_argument("files", nargs="+", help="list of yaml statistics files")

args = parser.parse_args()
time_range = [None, None]
local_asn = {}
peer_asn = {}
remote_asn = {}

# parse yaml statistics files
for fn in args.files:
  with open(fn, 'r') as fh:
    try:    
      data = yaml.safe_load(fh)
    except yaml.YAMLError as ex:
      print("cannot parse yaml file", ex)
      sys.exit(1)

    # extract org data
    meta = data["meta"]

    # get start of time range
    meta["from"] = datetime.datetime.fromisoformat(meta["from"])
    if time_range[0] is None or time_range[0] > meta["from"]:
      time_range[0] = meta["from"]

    # get end of time range
    meta["to"] = datetime.datetime.fromisoformat(meta["to"])
    if time_range[1] is None or time_range[1] < meta["to"]:
      time_range[1] = meta["to"]

    peer_asn[meta["asn"]] = meta

    # keep local_asn list
    if 'local_asn' in data:
      for asn, org in data['local_asn'].items():
        local_asn[asn] = {"org": org}

    # record peer stats
    for asn, stats in data["top_peers"].items():
      # skip statistics with zero values
      if 0 in stats.values():
        continue

      if asn not in remote_asn:
        remote_asn[asn] = stats
        remote_asn[asn]["local_peers"] = 1
      else:
        remote_asn[asn]["local_peers"] += 1
        for metric in stats.keys():
          if metric != "org":
            remote_asn[asn][metric] += stats[metric]

  # drop remote stats of peer_asn
  if asn in remote_asn:
    del(remote_asn[asn])

# format p95 values
cut_off_asn = []
for asn in remote_asn.keys():
  if remote_asn[asn]["in_p95"] < P95_CUT_OFF or remote_asn[asn]["out_p95"] < P95_CUT_OFF:
    cut_off_asn.append(asn)
  for k in ["in_p95", "out_p95"]:
    remote_asn[asn][f"{k}_si"] = format_bps(remote_asn[asn][k])

# prepare pug variables
pug_globals = {
  "local_asn": sorted({**local_asn, **peer_asn}.items(), key=lambda i: i[0]),
  "ts_from": time_range[0].strftime("%Y-%m-%d"),
  "ts_to": time_range[1].strftime("%Y-%m-%d"),
  "watermark": args.watermark,
}

# create individual asn stats
if args.individual:
  sibling_asn = []
  for org, url in SIBLING_ASN_SOURCES.items():
    with fsspec.open(f"filecache::{url}", mode='rt', filecache={'cache_storage': args.cache}) as fh:
      sibling_asn.append(sorted([int(asn.removeprefix("AS").strip()) for asn in fh.readlines()]))

  seen_siblings = []
  for asn in sorted(remote_asn.keys()):
    # skip if this asn was already a sibling
    if asn in seen_siblings:
      continue

    # check if we know any siblings
    siblings = [asn]
    for s in sibling_asn:
      if asn in s:
        siblings = s
        seen_siblings.extend(siblings)
        break

    # remote asn listed in this report
    report_asn = {asn: metrics for asn, metrics in remote_asn.items() if asn in siblings}

    # get total stats if more than a single asn is listed
    totals = {}
    if len(report_asn) > 1:
      for direction in ["in", "out"]:
        for metric in ["avg", "p95", "max"]:
          k = f"{direction}_{metric}"
          for a, stats in report_asn.items():
            if k in stats:
              if k in totals:
                totals[k] += stats[k]
              else:
                totals[k] = stats[k]

          totals[f"{k}_si"] = format_bps(totals[k])

    # generate single asn report
    html = pug_to_html("templates/single-asn.pug",
                   remote_asn=sorted(report_asn.items(), key=lambda i: i[0]),
                   totals=totals,
                   **pug_globals)
    write_report(html, f"DDIX_AS{asn}_{slugify(remote_asn[asn]['org'])}.pdf")

# remove ASN below P95 cut-off
for asn in cut_off_asn:
  del(remote_asn[asn])

html = pug_to_html("templates/top-asn.pug",
                   p95_cut_off=P95_CUT_OFF,
                   remote_asn=sorted(remote_asn.items(), key=lambda i: i[1]["in_p95"] + i[1]["out_p95"], reverse=True),
                   **pug_globals)
write_report(html=html, target="DDIX_Top-ASN.pdf", base_url=os.getcwd())
